# IT use case prioritization frameworks for 2025 programmes

The most effective approach to IT use case prioritization in 2025 combines **outcome-based value frameworks** with traditional scoring methodologies, creating a layered system that addresses strategic alignment, economic sequencing, and practical execution. The shift from project-based to value-realization prioritization represents the most significant evolution in the field, with organizations achieving **2-6x higher shareholder returns** when applying rigorous prioritization to digital and AI initiatives. WSJF (Weighted Shortest Job First) remains the gold standard for economic sequencing within SAFe environments, while RICE scoring dominates product-focused prioritization—but both are increasingly supplemented by AI-powered tools and new frameworks designed specifically for agentic AI and digital transformation programmes.

The critical insight for programme managers is that **no single framework suffices**. Best practice in 2025 involves a three-layer approach: strategic categorization using frameworks like MoSCoW or the Microsoft BXT model, followed by quantitative scoring through RICE or WSJF, validated by stakeholder consensus methods like Priority Poker. This layered methodology ensures both strategic alignment and economic optimization while maintaining stakeholder buy-in.

---

## Modern prioritization has shifted from project delivery to value realization

The fundamental transformation in IT prioritization since 2023 centers on measuring actual business outcomes rather than project completion metrics. Leading consultancies now recommend establishing a **Value Realization Office (VRO)** instead of traditional PMOs, creating what Capgemini calls an "infinite value flow" where value realization serves as both the starting point and goal of prioritization decisions.

Several new frameworks have emerged to address this shift. **Microsoft's BXT Framework** evaluates initiatives across three dimensions: Business strategy alignment, user eXperience desirability, and Technology feasibility, with each dimension scored 1-5 and plotted on a prioritization matrix yielding four action categories—Accelerate to MVP, Incubate for Testing, Reassess, or Deprioritize. **Deloitte's Differentiability Index** adds a crucial criterion for AI initiatives: whether a use case creates genuine competitive advantage versus commodity automation that competitors will also implement. This distinction between "table-stakes" and "differentiated" investments helps organizations avoid wasting resources on AI projects that won't generate lasting value.

Gartner's 2025 guidance introduces **Zero-Based Prioritization** as a periodic reset mechanism: conceptually placing all initiatives on hold and requiring explicit justification for what returns to the portfolio. This forces direct linkage to business priorities and eliminates legacy projects that continue through organizational inertia. Their TPESTRE trend-scanning approach—covering Technology, Political, Economic, Social, Trust/Ethics, Regulatory, and Environmental factors—recommends assessing external shocks and opportunities before internal prioritization begins.

For AI and ML implementation programmes specifically, the **GSAIF (Generative AI Strategic Investment Framework)** provides a two-phase approach: qualitative screening for feasibility and compliance, followed by weighted scoring on strategic alignment, technical readiness, ROI timeline, and risk factors. One e-commerce implementation using this framework achieved **3x improvement in customer retention**. McKinsey's Three Horizons framework, originally designed for growth strategy, has been adapted for AI prioritization, categorizing initiatives as H1 (optimize current operations, 0-12 months), H2 (enhance market positioning, 12-36 months), or H3 (transformational change, 36+ months).

---

## Traditional frameworks remain foundational but require strategic combination

Despite the emergence of new methodologies, established frameworks continue to dominate practical prioritization. The key is understanding when each applies and how to combine them effectively.

**WSJF (Weighted Shortest Job First)** serves as the core prioritization mechanism in SAFe environments. Its formula—Cost of Delay divided by Job Duration—incorporates User-Business Value, Time Criticality, and Risk Reduction/Opportunity Enablement, all estimated using relative Fibonacci scoring rather than absolute values. WSJF's fundamental strength lies in its economic focus: it automatically ignores sunk costs and promotes breaking large jobs into smaller batches. However, estimating Cost of Delay components requires significant stakeholder alignment and works best at the epic or capability level rather than for individual user stories.

**RICE scoring** (Reach × Impact × Confidence ÷ Effort), developed by Intercom's product team, provides the most data-driven approach when usage metrics are available. Reach quantifies affected users per time period; Impact uses a 0.25-3 scale from minimal to massive; Confidence acknowledges uncertainty with 50-100% ratings; and Effort measures person-months required. RICE's primary weakness is its tendency to **undervalue technical debt**, which typically shows low reach and impact despite being essential—a critical consideration for IT programmes with significant legacy system dependencies.

**MoSCoW** (Must-have, Should-have, Could-have, Won't-have) excels at stakeholder alignment and scope management, particularly for time-boxed projects with fixed deadlines. Its intuitive categories facilitate communication across technical and business stakeholders. The framework's limitation is the lack of ranking within categories—without supplementary scoring, all Must-haves appear equally urgent.

**The Value-Effort Matrix** (also called Impact-Effort or 2×2 prioritization) provides the fastest path to initial prioritization, mapping initiatives into four quadrants: Quick Wins (high value, low effort—do first), Big Bets (high value, high effort—plan carefully), Maybes (low value, low effort—do if time permits), and Time Sinks (low value, high effort—avoid). For early-stage planning and MVP identification, this visual approach builds consensus rapidly, though it lacks precision for complex portfolios.

The **Kano Model** offers unique customer-centric insight by categorizing features as Basic (expected—absence disappoints), Performance (linear satisfaction improvement), Attractive (delighters—unexpected features that create delight), Indifferent, or Reverse (features that cause dissatisfaction). Kano requires substantial customer research through paired questionnaires but prevents building features customers don't actually want—a common pitfall in IT programmes driven by internal assumptions.

The optimal combination follows a **layered approach**: use MoSCoW or Kano for initial strategic categorization, apply RICE or WSJF for detailed scoring within categories, validate effort assumptions through Priority Poker sessions, and confirm economic sequencing through Cost of Delay analysis for top candidates.

---

## Programme integration requires explicit flows from strategy to execution

Effective prioritization must flow seamlessly into Agile ceremonies, SAFe structures, and traditional PMO governance. The integration pattern follows a clear path: Strategic Vision → Portfolio Kanban (WSJF prioritization) → Epic Approval → ART Backlog → PI Planning → Sprint Backlog → Team Execution → Release.

Within **SAFe environments**, Portfolio Kanban visualizes epic flow through six states: Funnel (intake), Reviewing, Analyzing (Lean Business Case development), Portfolio Backlog (ready for implementation), Implementing, and Done. WSJF prioritization occurs during the Analyzing stage in collaboration with Business Owners, Enterprise Architects, and Product Management. The preparation timeline for PI Planning spans nine weeks: portfolio refinement begins nine weeks prior, ART-level epic-to-feature breakdown occurs at six weeks, and Product Owners align on prioritized features at three weeks before the PI Planning event.

For **Scrum integration**, prioritization flows through the Product Backlog—a continuously ordered list maintained by the Product Owner based on value, risk, and dependencies. Sprint Planning pulls highest-priority items based on team capacity, while Sprint Reviews create feedback loops that inform re-prioritization. Common techniques include stack ranking for forcing explicit trade-offs, T-shirt sizing for rapid effort estimation, and story points calculated through velocity tracking for capacity-based planning.

**Balancing quick wins against strategic initiatives** requires thinking of the IT portfolio like an investment portfolio. Quick wins require minimal resources yet yield visible, immediate benefits—they build organizational momentum, demonstrate value early, fund larger strategic projects, and overcome perception problems from past failures. Strategic initiatives spanning 12-18 months can fundamentally change market positioning but carry higher risk and longer payback periods. The practical approach involves maintaining deliberate portfolio diversification: use the Impact-Viability Matrix to score all projects, then ensure a healthy mix across short-term efficiency gains and long-term transformational investments. Organizations that adopt an either/or mindset—chasing only quick productivity gains or waiting for perfect strategic initiatives—consistently underperform those maintaining balanced portfolios.

**Dependency management** becomes critical after initial prioritization establishes sequence. Feature dependencies (complete one feature to enable ideal user experience), technical dependencies (code completion order for efficient development), resource dependencies (skills outside current team), and external dependencies (partner/vendor requirements) all constrain actual scheduling. SAFe addresses cross-team dependencies through the Program Board during PI Planning, with ROAM categorization (Resolved, Owned, Accepted, Mitigated) tracking dependency health. Best practice involves logging dependencies during planning, visualizing through dedicated Kanban lanes or Gantt relationships, accounting for resourcing implications, and initiating trade-off discussions when dependent items don't score high on standard criteria but remain essential.

---

## Evaluation criteria span four dimensions with industry-specific weighting

Comprehensive use case evaluation requires metrics across business value, technical feasibility, risk, and strategic alignment—with weights adjusted based on organizational and industry context.

**Business value metrics** include revenue impact (expected generation or protection), cost savings (operational efficiency gains), customer value (NPS, CSAT, retention improvements), market competitiveness (differentiation from competitors), user reach (quantified affected users per period), ROI/NPV (financial return calculations), and Cost of Delay (financial impact of postponement). The critical shift in 2025 involves supplementing these with **value velocity**—how quickly value can be realized—and **differentiability**—whether the initiative creates competitive advantage versus commodity capability.

**Technical feasibility criteria** encompass development effort (person-months), complexity (architecture and integration difficulty rated 1-5), resource availability (skills and capacity audit), infrastructure readiness (technology stack gaps), dependencies (reliance on other systems), maintainability (Total Cost of Ownership), and tech stack compatibility. For AI/ML programmes specifically, **data readiness** has become a critical criterion—whether the organization possesses adequate data foundations to support the initiative.

**Risk assessment factors** require evaluation of implementation risk (probability of failure), security risk (vulnerability exposure), compliance risk (regulatory penalty magnitude), schedule risk (timeline confidence percentage), resource risk (staff availability gaps), integration risk (system interconnection complexity), and change management risk (organizational adoption difficulty). Cloud migration programmes add **migration strategy risk** based on the 7 Rs framework: Rehost, Relocate, Replatform, Refactor, Repurchase, Retire, or Retain—each carrying different risk profiles.

**Strategic alignment measures** include goal alignment (connection to organizational OKRs), strategic fit score (weighted match to business priorities), stakeholder importance (priority to key decision-makers), and timing relevance (market or competitive urgency). The **ethical and trust dimension** has emerged as essential for AI initiatives, covering governance requirements, bias potential, and transparency obligations.

Industry-specific weightings apply across sectors. **Healthcare** programmes must prioritize HIPAA compliance and patient safety as filter criteria, with clinical outcomes weighted alongside financial metrics through Value of Investment frameworks—89% of healthcare executives identify digital transformation as a leading priority, with Revenue Cycle Management topping IT investments. **Financial services** organizations weight regulatory compliance (SOX, PCI DSS) as non-negotiable, prioritizing risk mitigation before customer experience improvements. **Manufacturing** programmes emphasize supply chain dependencies, OT security concerns, and equipment lifecycle considerations. **Retail** requires omnichannel timing alignment and customer experience weighting given direct revenue impact.

---

## Practical implementation requires appropriate tools, engagement techniques, and pitfall awareness

The tool landscape for IT prioritization has consolidated around several categories. **Enterprise portfolio management** platforms include Planview Portfolios for large enterprises requiring scenario modeling and resource optimization, Sciforma for PMOs needing what-if analysis, and Targetprocess for organizations aligning FinOps with portfolio management. **Product prioritization tools** like airfocus offer customizable frameworks with built-in RICE, MoSCoW, and WSJF scoring plus AI-assisted analysis; craft.io provides unlimited custom scoring fields; and ProductPlan emphasizes visual roadmapping for stakeholder communication. **Agile execution tools**—Jira, Azure DevOps, ClickUp, and Asana—integrate prioritization into sprint workflows through custom fields and priority matrices.

AI-enhanced capabilities in 2025 include **pattern recognition** for identifying recurring themes between requirements, **predictive analytics** using historical data to forecast outcomes and risks, **NLP** for extracting insights from unstructured feedback, and **automated scoring** assigning priorities based on predefined criteria. Forrester predicts organizations will **triple adoption of AIOps platforms** in 2025 to deliver contextually aware prioritization data.

**Stakeholder engagement techniques** range from Priority Poker (anonymous card-based voting eliminating the HiPPO—Highest Paid Person's Opinion—effect, proven **40% more accurate** than traditional estimation) to Buy-a-Feature (gamified approach using play money forcing trade-off decisions), dot voting for rapid initial prioritization, and T-shirt sizing for accessible complexity grouping. Workshop facilitation best practices include requiring pre-work review, establishing scoring scales and definitions upfront, limiting sessions to 1-2 hours, including subject matter experts for context, and using simultaneous reveal to prevent anchoring bias.

Common pitfalls demanding explicit mitigation include the **HiPPO effect** (counter with anonymous voting and objective frameworks), **shiny object syndrome** (validate against strategic goals before scoring), **"everything is urgent" syndrome** (force stack ranking—only one item can be number one), **static priorities** (establish weekly/bi-weekly review cadence), **underestimating complexity** (involve engineers early, use confidence scores, break down estimates), and **groupthink** (invite diverse perspectives, encourage dissenting opinions).

Case studies demonstrate these principles in practice. **Volvo Group** consolidated three investment portfolios totaling $1.87 billion using Planview Portfolios, achieving unified cross-portfolio dependency management and successfully renewing one-fifth of their portfolio annually. A leading **US insurance company** working with Infosys identified only 6 of 34 capability needs as truly critical, achieving approximately **33% reduction in Total Cost of Ownership** and 50% reduction in mainframe applications. A **P&C insurer** examined by MIT Sloan ranked 352 discrete projects against 6 weighted prioritization criteria, establishing an investment-decision mechanism that identified critical capabilities while discovering tactical savings from inappropriate billing practices.

---

## Conclusion: building an integrated prioritization capability

The most sophisticated IT programmes in 2025 treat prioritization not as a one-time exercise but as a continuous capability woven throughout programme governance. Three patterns distinguish high-performing organizations.

First, they **combine frameworks strategically** rather than seeking a single solution—using MoSCoW or BXT for initial categorization, RICE or WSJF for economic sequencing, and stakeholder consensus methods for validation. This layered approach captures strategic intent, economic optimization, and organizational buy-in simultaneously.

Second, they **embed prioritization in cadence**—PI Planning for quarterly strategic adjustment, sprint planning for continuous tactical prioritization, and weekly dependency reviews for execution optimization. Static annual prioritization fails to respond to market changes, technological shifts, or emerging risks.

Third, they **measure value realization rather than project completion**, tracking whether prioritized initiatives actually deliver projected business outcomes and using those insights to calibrate future prioritization accuracy. This feedback loop transforms prioritization from a planning exercise into an organizational learning capability.

For programme managers launching new prioritization initiatives, the recommended starting point is selecting 4-6 evaluation criteria weighted for organizational context, establishing a scoring framework (RICE for product-focused programmes, WSJF for SAFe environments), implementing regular review cadence, and documenting decision rationale for transparency and continuous improvement. The tools and techniques are mature; the differentiator is disciplined, consistent application integrated with execution governance.