# AI Use Case Prioritization: Consolidated Research Summary

## Document Control

| Property | Value |
|----------|-------|
| **Version** | 1.0 |
| **Date** | December 5, 2025 |
| **Status** | Consolidated Research |
| **Source Documents** | 7 research files from approach/research directory |
| **Purpose** | Single reference document consolidating all prioritization framework research |

---

## Executive Summary

This document consolidates research from multiple AI assistants (Claude Sonnet 4.5, Perplexity, ChatGPT-4, Gemini) on best practices for prioritizing AI/ML use cases in enterprise IT programmes for 2025. The research reveals strong consensus around several key frameworks, with **WSJF (Weighted Shortest Job First)** emerging as the gold standard for SAFe environments, complemented by **DVF (Desirability, Viability, Feasibility)** as a qualitative front-end filter and **Weighted Scoring Models** for custom strategic needs.

**Key Finding**: No single framework suffices. Best practice in 2025 involves a **three-layer approach**:
1. **Strategic Categorization**: DVF or MoSCoW for initial filtering
2. **Quantitative Scoring**: WSJF or RICE for economic sequencing
3. **Stakeholder Validation**: Priority Poker or consensus methods

---

## Table of Contents

1. [Framework Comparison Matrix](#framework-comparison-matrix)
2. [Detailed Framework Analysis](#detailed-framework-analysis)
3. [Integration with SAFe and Enterprise Practices](#integration-with-safe-and-enterprise-practices)
4. [2025-Specific Considerations](#2025-specific-considerations)
5. [Recommended Approach for BNZ](#recommended-approach-for-bnz)
6. [References](#references)

---

## Framework Comparison Matrix

### Primary Frameworks (Consensus Across All Sources)

| Framework | Primary Use | Scoring Method | SAFe Alignment | Best For | Key Limitation |
|-----------|-------------|----------------|----------------|----------|----------------|
| **WSJF (Weighted Shortest Job First)** | Economic sequencing | CoD ÷ Job Size | Native SAFe | Portfolio/Program prioritization | Requires relative estimation calibration |
| **DVF (Desirability, Viability, Feasibility)** | Qualitative filtering | 3-dimensional assessment | Complementary | Early-stage validation, innovation | No sequencing logic, subjective |
| **Weighted Scoring Model (WSM)** | Strategic alignment | Custom weighted criteria | Compatible | Custom strategic priorities | Requires consensus on weights |
| **RICE (Reach, Impact, Confidence, Effort)** | Product prioritization | Reach × Impact × Confidence ÷ Effort | Compatible | Product/feature level | Undervalues technical debt |
| **Value-Effort Matrix** | Visual prioritization | 2×2 quadrant | Compatible | Quick wins identification | Lacks precision for complex portfolios |
| **MoSCoW** | Scope management | Must/Should/Could/Won't | Compatible | Time-boxed projects | No ranking within categories |

### Emerging 2025 Frameworks

| Framework | Source | Innovation | Application |
|-----------|--------|------------|-------------|
| **GSAIF (Generative AI Strategic Investment Framework)** | Industry practice | Two-phase: qualitative screening → weighted scoring for AI | AI/ML programmes specifically |
| **Deloitte Differentiability Index** | Deloitte 2025 | Adds "uniqueness of AI value" dimension | Agentic AI and competitive differentiation |
| **Microsoft BXT (Business, eXperience, Technology)** | Microsoft | Maps to DVF, adds scoring matrix | Digital transformation initiatives |
| **DRICE (Data-driven RICE)** | Product practice | Adds revenue focus and readiness checks | Data-focused teams with clear metrics |
| **Zero-Based Prioritization** | Gartner 2025 | Periodic reset requiring re-justification | Portfolio rebalancing |

---

## Detailed Framework Analysis

### 1. WSJF (Weighted Shortest Job First) - The Enterprise Standard

**Consensus Rating**: ⭐⭐⭐⭐⭐ (All sources agree this is the gold standard for SAFe)

#### Formula
```
WSJF = Cost of Delay ÷ Job Size

Where Cost of Delay = User/Business Value + Time Criticality + Risk Reduction/Opportunity Enablement
```

#### Why It's the Standard

**Economic Optimization** (Source: All 4 research documents):
- Maximizes value delivered over time
- Mathematically proven to optimize economic flow
- Ignores sunk costs automatically

**Strategic Flexibility** (Source: Gemini research):
- **Risk Reduction (RR)**: Captures value of cybersecurity hardening, technical debt reduction, legacy system modernization
- **Opportunity Enablement (OE)**: Captures value of building platforms (Cloud, AI foundations) that enable future use cases
- This structure **forces prioritization of foundational work** alongside customer features

**Agile Integration** (Source: Claude & Gemini):
- Native to SAFe framework
- Uses relative estimation (Fibonacci) rather than precise financial calculations
- Scales across multiple Agile Release Trains (ARTs)

#### When WSJF Excels
- Large enterprise with multiple programs
- SAFe implementation environments
- Need to justify non-feature work (enablers, technical debt)
- Portfolio-level epic prioritization
- Requires economic rigor

#### Limitations
- **Subjectivity in relative scoring**: Requires stakeholder alignment on Cost of Delay components
- **Not intuitive for non-technical stakeholders**: Needs explanation and training
- **Doesn't handle hard dependencies**: Needs supplementary dependency mapping

**Best Practice** (Source: Perplexity & Gemini):
Score with 5-11 stakeholders per session, reset scores every 5 program increments to reflect changing priorities.

---

### 2. DVF (Desirability, Viability, Feasibility) - The Qualitative Filter

**Consensus Rating**: ⭐⭐⭐⭐ (Strong agreement as front-end filter, but not standalone)

#### Three Dimensions (IDEO Design Thinking Framework)

1. **Desirability**: Is it wanted by users/customers? Does it solve a real problem?
2. **Viability**: Is it financially viable? Does it fit the business model?
3. **Feasibility**: Can we build it with current technology and capabilities?

#### Integration Pattern (Source: Claude DVF research)

**DVF as Layer 1** (Qualitative screening):
```
Week 1: DVF Assessment
├─ Cross-functional workshop
├─ Score each use case 1-5 on each dimension
├─ Eliminate any scoring <3 on any dimension
└─ Create "viable candidates" list (must score 3+ on all dimensions)

Week 2-3: WSJF/RICE Quantitative Scoring
├─ Score only DVF-approved candidates
└─ Account for dependencies

Week 4: Portfolio Balancing
├─ Value-Effort Matrix visualization
├─ Ensure mix of quick wins + strategic bets
└─ Stakeholder validation (Priority Poker)
```

#### Strong Alignment with Microsoft BXT
(Source: Claude research)

DVF maps directly to BXT:
- **D**esirability → e**X**perience (user needs)
- **V**iability → **B**usiness (strategic/financial)
- **F**easibility → **T**echnology (capability/readiness)

#### Evolution: DVFS (Adding Sustainability)
(Source: Claude DVF research)

2025 frameworks increasingly add **Sustainability** as fourth dimension:
- Resource usage
- Environmental impact
- ESG compliance
- Particularly relevant for infrastructure and data center decisions

#### When DVF Excels
- Innovation and new product development
- Early-stage idea validation
- Cross-functional alignment workshops
- Filtering large idea backlogs (100+ ideas down to 20-30)
- Design thinking and transformation programs

#### Limitations (Source: Claude DVF research)
- **No sequencing logic**: Tells you what's viable, not what to do first
- **No dependency handling**: Doesn't account for technical prerequisites
- **Limited economic optimization**: Doesn't optimize for Cost of Delay
- **Subjective**: Relies on stakeholder judgment, can be politically influenced

---

### 3. Weighted Scoring Model (WSM) - Strategic Steering Mechanism

**Consensus Rating**: ⭐⭐⭐⭐ (All sources recommend for custom strategic priorities)

#### Six-Step Implementation (Source: Gemini research, validated by others)

1. **Define Criteria**: Cross-functional collaboration to identify value drivers
   - Examples: Customer Impact, Technical Feasibility, Strategic Fit, Revenue Potential, Risk Mitigation

2. **Assign Weights**: Percentages reflecting strategic priority (must sum to 100%)
   - This is where **executive strategic steering** happens

3. **Score Options**: Rate each use case on each criterion (typically 1-10 scale)
   - Use scoring guidelines to reduce bias

4. **Calculate Weighted Scores**: Multiply each score by weight, sum for total

5. **Analyze and Prioritize**: Rank by total score, consider dependencies

6. **Review and Refine**: Quarterly recalibration as strategy evolves

#### Strategic Steering Through Weight Manipulation (Source: Gemini)

**Example: Regulatory Compliance Year**
```
Risk Mitigation:        40%  (elevated for compliance push)
Business Value:         25%  (reduced temporarily)
Strategic Alignment:    20%
Technical Complexity:   15%  (inverse - lower complexity scores higher)
```

**Example: Growth/Innovation Year**
```
Business Value:         35%  (revenue focus)
Strategic Alignment:    25%
Risk Mitigation:        20%
Technical Complexity:   20%
```

This weight manipulation allows **top-down strategic mandate** while maintaining transparency.

#### When WSM Excels
- Organizations with frequently shifting strategic priorities
- Need to balance competing executive mandates
- Portfolio-level investment decisions
- Budget allocation across multiple themes
- Communicating priorities to Board

#### Limitations
- **Requires rigorous consensus**: If executives disagree on weights, the model breaks down
- **Can be gamed**: Teams learn to score favorably on high-weighted criteria
- **Maintenance overhead**: Requires quarterly recalibration

---

### 4. RICE (Reach, Impact, Confidence, Effort) - Product-Focused Standard

**Consensus Rating**: ⭐⭐⭐⭐ (Strong for product teams, less for enterprise IT)

#### Formula (Source: All sources)
```
RICE Score = (Reach × Impact × Confidence) ÷ Effort

Where:
Reach = Affected users per time period (quantified)
Impact = Value per user (scale: 0.25 = minimal, 0.5 = low, 1 = medium, 2 = high, 3 = massive)
Confidence = Certainty of estimates (percentage: 50-100%)
Effort = Person-months required
```

#### Key Innovation: Confidence Factor
The **Confidence** dimension is RICE's unique contribution, allowing teams to adjust for:
- Uncertain requirements
- New technology territory
- Lack of historical data
- Dependency on external factors

**Example** (Source: Perplexity):
```
Use Case A: High reach (10,000 users), high impact (2), but low confidence (50%) = Lower priority
Use Case B: Medium reach (5,000 users), medium impact (1), but high confidence (100%) = May rank higher
```

#### DRICE Evolution (Source: ChatGPT & Claude)

**D**ata-driven RICE adds:
- **Explicit revenue quantification**: Move from relative scoring to "$X expected revenue"
- **Readiness checks**: Is the use case "shovel-ready"?
- Focus on selecting items with highest confidence in delivery

#### When RICE Excels
- Product teams with clear user metrics
- Feature prioritization within a product
- When usage data is available (MAU, DAU, etc.)
- SaaS or digital product environments
- Need for data-driven justification

#### Limitations (Source: Atlassian framework research)
- **Undervalues technical debt**: Infrastructure work shows "low reach" despite being essential
- **Requires good data**: Without usage metrics, becomes subjective
- **Product-centric**: Less applicable to enterprise IT infrastructure projects

---

### 5. Value-Effort Matrix (Impact-Effort / 2×2) - The Communication Tool

**Consensus Rating**: ⭐⭐⭐⭐ (Universal agreement as visualization tool)

#### Four Quadrants (Source: All sources)

```
High Value, Low Effort     │  High Value, High Effort
"Quick Wins" - Do First    │  "Major Projects" - Plan Carefully
──────────────────────────┼────────────────────────────
Low Value, Low Effort      │  Low Value, High Effort
"Fill-ins" - Do if Time    │  "Time Sinks" - Avoid
```

#### Strategic Application (Source: Claude & ChatGPT)

**OpenAI's Enterprise Approach**: Score AI use cases by business value and implementation effort, resulting in:
- **Quick Wins**: Ideal starting points to build momentum (implement Wave 1)
- **Strategic Bets**: High-value transformational use cases (defer to Wave 2/3 after quick wins prove value)
- **Self-Service**: Low-priority individuals/team solutions (not programme-level)
- **Avoid**: De-prioritize or put on hold

#### Integration with WSJF (Source: Claude research)
The matrix provides **visual validation** of WSJF results:
- High WSJF scores should cluster in "Quick Wins" quadrant
- If high WSJF items appear in "Major Projects," investigate job size estimates

#### When It Excels
- Executive presentations (simple visual)
- Initial portfolio triage (500 ideas → 50 candidates)
- Cross-functional alignment workshops
- Building momentum argument (show quick wins exist)

#### Limitations
- **Only 2 dimensions**: Misses strategic alignment, risk, time criticality
- **Subjective placement**: Teams can debate where items sit on axes
- **No sequencing within quadrants**: All "quick wins" appear equal priority

---

### 6. MoSCoW (Must, Should, Could, Won't) - The Scope Management Standard

**Consensus Rating**: ⭐⭐⭐ (Good for stakeholder alignment, but needs supplementation)

#### Four Categories (Source: Atlassian & Perplexity)

- **Must Have**: Critical for delivery, non-negotiable
- **Should Have**: Important but not vital, workaround exists
- **Could Have**: Desirable but not necessary, nice-to-have
- **Won't Have**: Out of scope for this release

#### Integration Pattern (Source: Claude research)

MoSCoW works best in **time-boxed contexts**:
1. **Define fixed deadline** (e.g., regulatory go-live date)
2. **MoSCoW categorization** with stakeholders
3. **WSJF scoring within "Must Have"** category to determine sequence
4. Commit to Must Have, plan for Should Have, drop Could Have if needed

#### When It Excels
- Fixed-deadline projects (regulatory, contractual)
- Stakeholder scope negotiation
- Communicating trade-offs to non-technical audiences
- Requirements management in waterfall-to-agile transitions

#### Limitations
- **No ranking within categories**: All "Must Haves" appear equal
- **Prone to scope creep**: Everything becomes "Must Have" without discipline
- **Lacks economic optimization**: Doesn't consider Cost of Delay

---

## Integration with SAFe and Enterprise Practices

### SAFe Portfolio Kanban Flow (Source: Perplexity & Gemini)

```
Funnel (Intake)
    ↓
Reviewing (Initial Assessment - Use DVF here)
    ↓
Analyzing (Business Case Development - Calculate WSJF)
    ↓
Portfolio Backlog (WSJF-ordered, ready for PI Planning)
    ↓
Implementing (ARTs pull into PIs based on capacity)
    ↓
Done (Value delivered)
```

**Key Integration Point** (Source: Gemini):
WSJF calculation happens during **Analyzing** stage, with collaboration between:
- Business Owners (User/Business Value, Time Criticality)
- Enterprise Architects (Risk Reduction, Feasibility)
- Product Management (Job Size estimation)

### Program Increment (PI) Planning Integration (Source: All sources)

**9-Week PI Planning Preparation Timeline**:

```
Week -9: Portfolio Refinement
├─ WSJF scoring of new epics
└─ Dependency identification begins

Week -6: Epic-to-Feature Breakdown
├─ ARTs decompose epics into features
└─ Cross-ART dependency mapping

Week -3: Feature Prioritization
├─ Product Owners align on WSJF-prioritized features
└─ Capacity estimates

Week 0: PI Planning Event
├─ Teams commit to PI Objectives
├─ ART Planning Board visualizes dependencies
└─ Confidence votes captured
```

**Critical Success Factor** (Source: Gemini):
**Dependency Management** is the hard constraint that transforms WSJF-ranked backlog into a realistic PI schedule. High-WSJF items cannot be scheduled if prerequisites are missing.

### Dependency Management Framework (Source: Gemini - most detailed)

**Four Dependency Types**:
1. **Feature Dependencies**: Complete Feature A to enable ideal UX for Feature B
2. **Technical Dependencies**: Code completion order for efficient development
3. **Resource Dependencies**: Skills/people outside current team
4. **External Dependencies**: Partner/vendor requirements

**ROAM Classification** (during PI Planning):
- **R**esolved: Dependency eliminated
- **O**wned: Team takes ownership
- **A**ccepted: Acknowledged, mitigation plan exists
- **M**itigated: Actions reduce impact

**Tooling** (Source: Gemini - Azure Boards SAFe implementation):
- Link work items across ARTs
- Shared iteration paths (PI alignment)
- Custom dependency tracking fields
- ART Planning Board visualization

---

## 2025-Specific Considerations

### Strategic Investment Themes (Source: Gemini - Capgemini 2025 report)

2025 IT portfolios must allocate to five dominant themes:
1. **Cloud Optimization**
2. **Customer Experience**
3. **Cybersecurity** (elevated to core business imperative)
4. **Data & AI Foundations**
5. **Intelligent Industry** (automation, IoT)

**Implication for Prioritization**:
Use case portfolios should be **tagged** with these themes, and WSM weights adjusted quarterly to align with strategic theme emphasis.

### AI/ML-Specific Considerations (Source: ChatGPT & Perplexity)

**Three Critical Filters for AI Use Cases**:

1. **Data Readiness** (Source: ChatGPT):
   - Is adequate data available?
   - Is data quality sufficient?
   - Are data pipelines established?
   - Assessment: Red/Yellow/Green gate before prioritization

2. **AI Differentiability** (Source: ChatGPT - Deloitte 2025):
   - Does AI provide unique value vs. traditional solution?
   - Will it create competitive advantage?
   - "Table stakes" vs. "Differentiated" classification
   - Score: 1-5 (add as dimension to WSM)

3. **Ethical & Governance Requirements** (Source: Perplexity - EY framework):
   - Bias potential
   - Regulatory compliance (GDPR, model risk management)
   - Explainability requirements
   - Assessment: Mandatory governance gate

### Cybersecurity & AI Integration (Source: Gemini)

**2025 Mandate**: AI/ML cybersecurity use cases must be explicitly prioritized in RR/OE component of WSJF.

**Rationale**:
- Cyber threats evolving faster than traditional security
- AI-driven threat detection is now **essential** for adaptive security posture
- Ransomware and zero-day attacks require ML-powered defenses
- Regulatory requirements mandate AI security capabilities

**Prioritization Impact**:
Use cases like "AI-powered SIEM," "Behavioral anomaly detection," or "Automated incident response" receive **high RR scores** in WSJF, ensuring they compete effectively against revenue-generating features.

---

## Recommended Approach for BNZ

### Multi-Framework Integrated Model

Based on consensus across all sources and BNZ's context (24 AI use cases, enterprise scale, SAFe alignment preference), the recommended approach is:

### Phase 1: Qualitative Filtering (Week 1-2)

**Framework: DVF (Desirability, Viability, Feasibility)**

**Process**:
1. **Workshop**: Cross-functional team (Business Owners, Enterprise Architects, Product Managers, Risk & Compliance)
2. **Scoring**: Each of 24 use cases scored 1-5 on each DVF dimension:
   - **Desirability**: Customer/user demand, problem-solution fit
   - **Viability**: Financial viability, ROI, strategic alignment
   - **Feasibility**: Technical capability, data readiness, resource availability
3. **Threshold**: Eliminate any use case scoring <3 on any dimension
4. **Output**: "Viable Candidates" list (expect 15-20 use cases)

**Rationale**: Prevents wasting time scoring infeasible use cases with quantitative methods.

---

### Phase 2: Quantitative Prioritization (Week 3-4)

**Framework: WSJF (Weighted Shortest Job First)**

**Process**:
1. **Relative Estimation Workshop**: 5-11 stakeholders per session
2. **Score Components** (Modified Fibonacci: 1, 2, 3, 5, 8, 13, 20):

   **Cost of Delay Components**:
   - **User/Business Value**:
     - Revenue impact (increased revenue, retention)
     - Cost savings (FTE reduction, operational efficiency)
     - Customer experience improvement
     - Score: 1-20

   - **Time Criticality**:
     - Regulatory deadlines
     - Competitive windows
     - Contractual obligations
     - Market timing
     - Score: 1-20

   - **Risk Reduction / Opportunity Enablement**:
     - **RR**: Cybersecurity hardening, technical debt reduction, compliance risk
     - **OE**: Platform foundation (AI, Cloud, Data) enabling future use cases
     - Score: 1-20

   **Job Size**:
   - Effort estimate (person-months, team-months, or simply small/medium/large)
   - Score: 1-20

3. **Calculate WSJF**: (User/Business Value + Time Criticality + RR/OE) ÷ Job Size
4. **Rank**: Highest WSJF = highest priority
5. **Output**: WSJF-ranked backlog of viable use cases

**Rationale**: WSJF is SAFe-native, mathematically sound for economic optimization, and forces justification of foundational work (AI governance platforms, feature stores) through RR/OE scoring.

---

### Phase 3: Strategic Validation (Week 5-6)

**Framework: Weighted Scoring Model (WSM)**

**Purpose**: Validate WSJF results against BNZ's strategic mandates and adjust for factors WSJF may under-emphasize.

**Process**:
1. **Define Strategic Criteria** (example for BNZ 2025):
   - Business Value & Growth: 30%
   - Time & Regulatory Urgency: 25%
   - Risk Mitigation & AI Foundation: 25% (elevated for 2025 governance focus)
   - Implementation Complexity: 20% (inverse - favor shorter jobs)

2. **Score Each Use Case**: 1-10 on each criterion
3. **Calculate Weighted Score**: Sum of (Score × Weight)
4. **Compare with WSJF Ranking**: Look for significant discrepancies

**Expected Outcome**: WSM should largely validate WSJF, with minor reordering based on strategic weight differences.

**Conflict Resolution**: If WSM and WSJF significantly disagree, convene executive review to decide.

---

### Phase 4: Portfolio Balancing & Visualization (Week 7)

**Framework: Value-Effort Matrix**

**Process**:
1. **Plot**: Top 15-20 use cases on 2×2 matrix (Value = WSJF CoD, Effort = Job Size)
2. **Identify Quadrants**:
   - Quick Wins (High Value, Low Effort) → Wave 1 (Months 1-9)
   - Strategic Bets (High Value, High Effort) → Wave 2-3 (Months 10-30)
   - Fill-ins (Low Value, Low Effort) → Backlog
   - Avoid (Low Value, High Effort) → De-prioritize

3. **Balance Check**: Ensure portfolio has:
   - 40-50% Quick Wins (demonstrate early value)
   - 40-50% Strategic Bets (long-term transformation)
   - 10-20% Enabling infrastructure (even if low immediate value)

4. **Output**: Visual roadmap for executive presentation

---

### Phase 5: Dependency-Based Scheduling (Week 8-9)

**Framework: Dependency Mapping + PI Planning Preparation**

**Process**:
1. **Map Dependencies**: For top 12-15 use cases:
   - Technical: "[UC-013](../../use-cases/UC-013/UC-013-Fraud-Ops-v1.0.0.md) Fraud requires [UC-009](../../use-cases/UC-009/UC-009-Data-Products-v1.0.0.md) Data Products feature store"
   - Resource: "[UC-010](../../use-cases/UC-010/UC-010-SDLC-v1.0.0.md) SDLC requires specialized DevOps team"
   - External: "[UC-011](../../use-cases/UC-011/UC-011-Fincrime-v1.0.0.md) Fincrime requires vendor API availability"

2. **Sequence Constraints**: Adjust WSJF order for dependency reality
   - If [UC-009](../../use-cases/UC-009/UC-009-Data-Products-v1.0.0.md) (Data Products) enables 5 other use cases, it may move to Wave 1 even if WSJF is moderate

3. **Wave Allocation**:
   - **Wave 1 (Q1-Q3 2026)**: 5 use cases + platform infrastructure
   - **Wave 2 (Q4 2026-Q3 2027)**: 8-12 use cases leveraging Wave 1 platforms
   - **Wave 3 (Q4 2027-Q2 2028)**: Remaining 7 use cases

4. **PI Planning Input**: Create PI-ready backlog with features decomposed from epics

5. **Output**: Dependency-sequenced, capacity-validated roadmap

---

### Ongoing: Quarterly Recalibration

**Process**:
1. **Review Triggers**:
   - End of each PI (every 10-12 weeks)
   - Strategic priority shifts
   - Regulatory changes
   - Technology breakthroughs or failures

2. **Recalibration Activities**:
   - Re-score WSJF for top 10 use cases
   - Adjust WSM weights if strategy changes
   - Re-run DVF for new use case proposals
   - Update dependency maps

3. **Governance**: AI Programme Board approval for major re-prioritization

---

## Summary Recommendation: Hybrid Model

### For BNZ's 24 AI Use Cases

**Week 1-2**: **DVF Filtering** (eliminate infeasible use cases)
↓
**Week 3-4**: **WSJF Scoring** (economic prioritization)
↓
**Week 5-6**: **WSM Validation** (strategic alignment check)
↓
**Week 7**: **Value-Effort Matrix** (portfolio balancing)
↓
**Week 8-9**: **Dependency Sequencing** (realistic scheduling)
↓
**Ongoing**: **Quarterly Recalibration** (dynamic adjustment)

### Why This Model Works

1. **DVF**: Filters out non-viable ideas early (saves time)
2. **WSJF**: Provides economic rigor and forces justification of enablers
3. **WSM**: Allows executive strategic steering through weight adjustment
4. **Value-Effort**: Creates compelling visual story for stakeholders
5. **Dependencies**: Ensures realistic schedule, prevents failures

### Tooling Recommendation

- **Spreadsheet** (Phase 1-3): Excel/Google Sheets for scoring workshops
- **Azure Boards** (Phase 4-5): SAFe configuration for dependency management, PI planning
- **PowerBI/Tableau** (Ongoing): Dashboard for executive visibility

---

## References

### Source Documents

1. **01.02-prioritisation-frameworks-claude-sonnet-4.5.md**: Comprehensive analysis of modern frameworks, three-layer approach, emphasis on value realization
2. **01.03-prioritisation-frameworks-perplexity.md**: Deep dive on WSJF, Lean Portfolio Management, SAFe integration, dependency management
3. **01.04-prioritisation-frameworks-chatgpt.md**: AI/ML-specific frameworks (GSAIF, Deloitte Differentiability, BCG, KPMG), vendor perspectives
4. **01.05-prioritisation-frameworks-gemini.md**: Academic rigor on WSJF, WSM implementation, risk prioritization, PI Planning integration
5. **02.02-use-of-dvf-claude-sonnet-4.5.md**: Detailed DVF analysis, integration patterns, DVF's role as Layer 1

### Key External References (From Research)

**Frameworks & Standards**:
- Scaled Agile Framework (SAFe) 6.0 - WSJF methodology
- IDEO Design Thinking - DVF framework
- Microsoft BXT Framework
- Deloitte Agentic AI Framework (2025)
- Gartner AI Opportunity Radar (2025)
- Forrester AI Use Case Selection Framework (2024-2025)

**Industry Reports**:
- Capgemini: "Investment Trends 2025" (Cloud, Customer Experience, Cybersecurity, AI themes)
- McKinsey: "Extracting Value from AI in Banking" (2025)
- BCG: "AI Opportunity Assessment" (automation potential, ROI focus)
- KPMG: "AI Investment Scorecard" (cost-benefit analysis)
- Gartner: "Zero-Based Prioritization" (2025)

**Academic & Practitioner Sources**:
- Dr. Noriaki Kano: "Kano Model" (customer satisfaction analysis)
- Intercom: "RICE Framework" (product prioritization)
- Statsig: "DRICE Framework" (data-driven evolution)
- Atlassian: "Agile Prioritization Frameworks"
- ProductPlan: "WSJF and Value vs. Complexity guides"

**Tools & Platforms**:
- Azure DevOps/Azure Boards (SAFe implementation)
- Jira (Agile prioritization)
- airfocus (AI-assisted prioritization)
- Planview Portfolios (enterprise portfolio management)

---

## Appendix: Quick Reference Cheat Sheet

### When to Use Which Framework

| Situation | Use This Framework | Why |
|-----------|-------------------|-----|
| SAFe environment, portfolio-level | **WSJF** | Native to SAFe, economic optimization |
| Early-stage idea validation | **DVF** | Qualitative filter before spending time on detailed scoring |
| Custom strategic mandate | **WSM** | Allows executive steering via weights |
| Product team, clear user metrics | **RICE** | Data-driven, quantifiable reach and impact |
| Quick executive alignment | **Value-Effort Matrix** | Simple visual, builds consensus fast |
| Fixed-deadline scope negotiation | **MoSCoW** | Good for stakeholder scope management |
| AI/ML competitive differentiation | **Deloitte Differentiability Index** | Focuses on unique value of AI |
| Innovation portfolio | **Gartner AI Opportunity Radar** | Strategic opportunity mapping |

### Red Flags (When NOT to Use)

| Framework | Don't Use If... |
|-----------|-----------------|
| **WSJF** | Team hasn't been trained, no stakeholder alignment on relative scoring, not using SAFe |
| **DVF** | Need precise sequencing, have hard dependencies, purely engineering-focused decisions |
| **WSM** | Can't get executive agreement on weights, frequent political disagreements |
| **RICE** | Don't have user metrics, working on infrastructure not products, undervalues enablers |
| **MoSCoW** | Need economic optimization, no fixed deadline, everything becomes "Must Have" |

---

*This consolidated research document represents best practices from multiple AI assistants and industry sources as of December 2025. Frameworks should be adapted to organizational context and culture.*
