# Embedded AI Capability Area: Capabilities and Sub-Capabilities

**Capability Area Acronym:** EB (Embedded AI)

## Document Control

| Property | Value |
|----------|-------|
| **Version** | 1.0 |
| **Status** | Active |
| **Last Updated** | October 5, 2025 |
| **Owner** | BNZ Technology Strategy & Architecture |
| **Applies To** | AI Platform - Embedded AI Capability Area |
| **Review Cycle** | Quarterly |

## Purpose

This document provides comprehensive definitions for all capabilities and sub-capabilities within the Embedded AI Capability Area of the BNZ AI Platform. It establishes a common vocabulary for **accelerating AI value delivery** by safely enabling AI features embedded in SaaS platforms and third-party solutions, while maintaining appropriate vendor oversight and regulatory compliance.

**Strategic Intent**: Unlock the 70%+ of AI innovation happening in SaaS platforms (Microsoft 365 Copilot, Salesforce Einstein, ServiceNow AI, Amazon Connect) faster than building equivalent capabilities in-house, while ensuring APRA CPS 230 material service provider oversight and NZ Privacy Act compliance.

## Scope

This document covers:
- Capabilities for rapid, safe enablement of embedded AI features
- Vendor assessment and onboarding acceleration
- SaaS AI feature evaluation and activation
- Material service provider oversight (APRA CPS 230)
- Value realization and ROI tracking
- Technology components and integration patterns

---

## Capability Hierarchy Overview

The Embedded AI Capability Area is organized into **seven primary capabilities** focused on value acceleration:

```
Embedded AI Capability Area
├── 1. SaaS AI Discovery & Evaluation
│   ├── 1.1 Feature Discovery & Cataloging
│   ├── 1.2 Business Value Assessment
│   ├── 1.3 Technical Feasibility Analysis
│   └── 1.4 ROI & Cost-Benefit Analysis
├── 2. Vendor Enablement & Onboarding
│   ├── 2.1 Accelerated Vendor Assessment (Risk-Tiered)
│   ├── 2.2 Material Service Provider Classification (APRA CPS 230)
│   ├── 2.3 Contractual Enablement & SLAs
│   └── 2.4 Technical Integration Readiness
├── 3. Feature Activation & Deployment
│   ├── 3.1 Rapid Pilot & Sandbox Environments
│   ├── 3.2 Feature Configuration & Customization
│   ├── 3.3 Enterprise Integration & SSO
│   ├── 3.4 Staged Rollout Management
│   └── 3.5 User Adoption & Training
├── 4. Value Realization & Optimization
│   ├── 4.1 Usage Analytics & Insights
│   ├── 4.2 ROI Measurement & Tracking
│   ├── 4.3 Performance Optimization
│   ├── 4.4 Feature Evolution & Upgrades
│   └── 4.5 Continuous Business Case Validation
├── 5. Operational Oversight (Light-Touch)
│   ├── 5.1 Vendor Performance Monitoring
│   ├── 5.2 Service Level Compliance
│   ├── 5.3 Change Impact Assessment
│   └── 5.4 Incident Coordination
├── 6. Data & Privacy Enablement
│   ├── 6.1 Data Access & Permissions
│   ├── 6.2 Privacy Impact Assessment (NZ Privacy Act)
│   ├── 6.3 Cross-Border Data Handling
│   └── 6.4 Data Quality for AI Inputs
└── 7. Portfolio & Ecosystem Management
    ├── 7.1 SaaS AI Portfolio View
    ├── 7.2 Vendor Relationship Management
    ├── 7.3 Ecosystem Integration Patterns
    └── 7.4 Strategic Vendor Partnerships
```

---

## 1. SaaS AI Discovery & Evaluation

**Definition**: The capability to systematically discover, evaluate, and prioritize AI features embedded in SaaS platforms based on business value, technical feasibility, and strategic fit.

**Purpose**: Accelerate time-to-value by identifying high-impact embedded AI opportunities and fast-tracking evaluation processes.

**Strategic Importance**: Enables BNZ to capitalize on vendor R&D investments and bring AI capabilities to market 3-6x faster than building in-house.

### 1.1 Feature Discovery & Cataloging

**Definition**: Continuously discover and catalog AI features available in existing and prospective SaaS platforms.

**Key Components**:
- **Feature Scanning**: Automated monitoring of vendor product releases and AI roadmaps
- **Capability Mapping**: Map SaaS AI features to BNZ business capabilities
- **Feature Catalog**: Centralized repository of available embedded AI features
- **Vendor Announcements**: Track major AI feature launches (Microsoft Ignite, Salesforce Dreamforce, AWS re:Invent)

**Technologies**:
- Vendor API integrations
- Release note monitoring tools
- Feature catalog platforms
- Integration with BNZ capability model

**Maturity Indicators**:
- **Basic**: Manual tracking of vendor announcements
- **Intermediate**: Automated catalog with quarterly updates
- **Advanced**: Real-time feature discovery with impact assessment
- **Optimized**: Predictive feature value scoring and auto-prioritization

**Success Metrics**:
- Number of features cataloged
- Time from vendor release to internal evaluation (days)
- Feature-to-capability coverage (%)
- Discovery automation rate (%)

---

### 1.2 Business Value Assessment

**Definition**: Rapidly assess business value and strategic alignment of discovered embedded AI features.

**Key Components**:
- **Value Scoring Framework**: Standardized scoring for business impact
- **Use Case Identification**: Map features to specific business problems
- **Stakeholder Input**: Capture business unit needs and pain points
- **Strategic Alignment**: Assess fit with BNZ AI strategy and priorities
- **Competitive Analysis**: Benchmark against peer bank AI adoption

**Technologies**:
- Value assessment frameworks
- Stakeholder collaboration tools
- Business case templates
- Competitive intelligence platforms

**Maturity Indicators**:
- **Basic**: Ad-hoc business case development
- **Intermediate**: Standardized value assessment framework
- **Advanced**: Rapid value scoring with stakeholder engagement
- **Optimized**: AI-assisted value prediction and prioritization

**Success Metrics**:
- Average assessment cycle time (days)
- % of features with documented business case
- Value realization accuracy (actual vs projected)
- Stakeholder engagement rate (%)

---

### 1.3 Technical Feasibility Analysis

**Definition**: Assess technical viability of integrating and deploying embedded AI features within BNZ's technology landscape.

**Key Components**:
- **Integration Complexity**: Evaluate API, data, and SSO integration requirements
- **Data Readiness**: Assess availability and quality of required data
- **Security Assessment**: Identity security and compliance implications
- **Performance Requirements**: Evaluate latency, throughput, and scalability needs
- **Dependency Mapping**: Identify technical dependencies and prerequisites

**Technologies**:
- Integration assessment tools
- API documentation analysis
- Security scanning platforms
- Architecture modeling tools

**Maturity Indicators**:
- **Basic**: Manual technical review by architecture team
- **Intermediate**: Standardized technical assessment checklist
- **Advanced**: Automated feasibility analysis with risk scoring
- **Optimized**: Predictive integration complexity modeling

**Success Metrics**:
- Technical assessment cycle time (days)
- Integration success rate (%)
- Post-deployment issue rate (#)
- Architecture standard compliance (%)

---

### 1.4 ROI & Cost-Benefit Analysis

**Definition**: Calculate expected return on investment and conduct comprehensive cost-benefit analysis for embedded AI features.

**Key Components**:
- **Cost Modeling**: Licensing, implementation, training, and operational costs
- **Benefit Quantification**: Productivity gains, cost savings, revenue impact
- **ROI Calculation**: Net present value, payback period, IRR
- **Risk-Adjusted Returns**: Factor in implementation and operational risks
- **Comparison to Build**: Compare to cost/timeline of building equivalent capability

**Technologies**:
- Financial modeling tools
- ROI calculators
- Benchmarking databases
- Portfolio management platforms

**Maturity Indicators**:
- **Basic**: High-level cost estimates and benefit assumptions
- **Intermediate**: Detailed financial models with sensitivity analysis
- **Advanced**: Risk-adjusted ROI with probabilistic modeling
- **Optimized**: Real-time ROI tracking with continuous validation

**Success Metrics**:
- Average ROI (%)
- Payback period (months)
- Build vs buy time advantage (%)
- ROI forecast accuracy (%)

---

## 2. Vendor Enablement & Onboarding

**Definition**: The capability to rapidly yet safely onboard SaaS vendors providing embedded AI, balancing speed-to-value with APRA CPS 230 material service provider requirements.

**Purpose**: Reduce vendor onboarding time from months to weeks while maintaining regulatory compliance and risk management standards.

**Strategic Importance**: Removes procurement and compliance friction as a barrier to AI innovation, enabling rapid experimentation and deployment.

### 2.1 Accelerated Vendor Assessment (Risk-Tiered)

**Definition**: Conduct risk-based vendor assessments using tiered approaches that match assessment depth to risk level and business criticality.

**Key Components**:
- **Risk Tiering**: Classify vendors into Tier 1-4 based on criticality and data sensitivity
- **Streamlined Questionnaires**: Pre-approved, AI-specific assessment questions
- **Vendor Profiles**: Maintain profiles for major SaaS vendors (Microsoft, Salesforce, AWS, Google)
- **Fast-Track Process**: Expedited review for low-risk, non-customer-facing use cases
- **Third-Party Assessments**: Leverage SOC 2, ISO certifications, and FS-ISAC assessments

**Technologies**:
- Vendor risk management platforms
- Assessment automation tools
- Third-party certification registries
- Risk scoring engines

**Maturity Indicators**:
- **Basic**: Full assessment for all vendors (3-6 months)
- **Intermediate**: Risk-tiered assessment with fast-track (6-8 weeks)
- **Advanced**: Pre-approved vendor profiles with incremental reviews (2-4 weeks)
- **Optimized**: Automated risk scoring with continuous monitoring (1-2 weeks)

**Success Metrics**:
- Average vendor onboarding time by tier (days)
- % of vendors using fast-track process
- Assessment reuse rate (%)
- Vendor approval success rate (%)

---

### 2.2 Material Service Provider Classification (APRA CPS 230)

**Definition**: Classify SaaS AI vendors as material service providers under APRA CPS 230 when supporting critical operations, ensuring appropriate oversight without blocking innovation.

**Key Components**:
- **Materiality Criteria**: Assessment against CPS 230 materiality factors
- **Critical Operations Mapping**: Link vendors to critical banking operations
- **Enhanced Due Diligence**: Additional requirements for material providers
- **APRA Notification**: 20-day notification for material arrangements
- **Board Reporting**: Material provider registers and risk reporting
- **Exit Planning**: Contingency plans for critical vendor services

**Technologies**:
- GRC platforms with CPS 230 templates
- Critical operations mapping tools
- Vendor register systems
- Board reporting dashboards

**Maturity Indicators**:
- **Basic**: Manual materiality assessment, reactive classification
- **Intermediate**: Documented criteria with consistent application
- **Advanced**: Automated materiality scoring with proactive monitoring
- **Optimized**: Predictive materiality analysis and dynamic classification

**Success Metrics**:
- % of vendors with materiality assessment
- Material provider register completeness (%)
- APRA notification timeliness (%)
- Exit plan coverage (%)

**APRA CPS 230 Requirements**:
- Material service providers supporting critical operations require enhanced oversight
- 20-day notification requirement for new/changed material arrangements
- Formal agreements with audit rights for APRA
- Business continuity plans including vendor failure scenarios
- Annual testing of continuity arrangements

---

### 2.3 Contractual Enablement & SLAs

**Definition**: Streamline contract negotiations and establish appropriate service level agreements that enable rapid deployment while protecting BNZ interests.

**Key Components**:
- **Pre-Negotiated Terms**: Standard terms for common SaaS AI vendors
- **AI-Specific Clauses**: Model transparency, data usage rights, IP ownership
- **SLA Definition**: Performance, availability, and support requirements
- **Data Protection**: NZ Privacy Act compliance and cross-border transfer safeguards
- **Liability & Indemnification**: Shared accountability for AI decisions
- **Exit Provisions**: Data portability and service termination rights

**Technologies**:
- Contract lifecycle management systems
- Template libraries
- SLA monitoring platforms
- Legal review automation

**Maturity Indicators**:
- **Basic**: Bespoke contract negotiation (3-6 months)
- **Intermediate**: Template-based with standard terms (6-8 weeks)
- **Advanced**: Pre-approved vendor agreements with addendums (2-4 weeks)
- **Optimized**: Click-through for pre-approved vendors (days)

**Success Metrics**:
- Average contract cycle time (days)
- % of contracts using standard terms
- SLA achievement rate (%)
- Contract dispute rate (%)

---

### 2.4 Technical Integration Readiness

**Definition**: Prepare technical infrastructure and integration patterns to enable rapid deployment of embedded AI features.

**Key Components**:
- **Integration Patterns Library**: Pre-built patterns for common integrations (SSO, APIs, data sync)
- **Sandbox Environments**: Isolated environments for safe testing
- **API Management**: Standardized API gateway and security controls
- **Data Pipelines**: Pre-configured data flows for common scenarios
- **Identity Integration**: SSO and role-based access control setup
- **Monitoring Instrumentation**: Observability for embedded AI systems

**Technologies**:
- API gateways (Apigee, Kong)
- Integration platforms (MuleSoft, Dell Boomi)
- Identity providers (Azure AD, Okta)
- Observability platforms (Datadog, Splunk)
- Sandbox/development environments

**Maturity Indicators**:
- **Basic**: Custom integration per vendor (8-12 weeks)
- **Intermediate**: Reusable integration patterns (4-6 weeks)
- **Advanced**: Self-service integration platform (1-2 weeks)
- **Optimized**: Auto-provisioning with API-driven configuration (days)

**Success Metrics**:
- Average integration time (days)
- Integration pattern reuse rate (%)
- Integration failure rate (%)
- Time to first API call (hours)

---

## 3. Feature Activation & Deployment

**Definition**: The capability to rapidly activate, configure, and deploy embedded AI features with controlled rollouts and strong adoption support.

**Purpose**: Minimize time from vendor contract to production value, with safe staged rollouts and high user adoption.

**Strategic Importance**: Converts evaluated opportunities into business value, with deployment speed as competitive advantage.

### 3.1 Rapid Pilot & Sandbox Environments

**Definition**: Provision isolated environments for rapid experimentation and pilot programs before full production deployment.

**Key Components**:
- **Instant Provisioning**: Self-service sandbox creation for business users
- **Pre-Production Environments**: Scaled environments for pilot programs
- **Test Data Management**: Representative data for realistic testing
- **Pilot User Groups**: Curated user groups for early feedback
- **Evaluation Frameworks**: Structured approaches to assess pilot success
- **Fail-Fast Mechanisms**: Quick decision points to continue or terminate pilots

**Technologies**:
- Cloud provisioning automation
- Test data management tools
- User feedback platforms
- Pilot program management tools

**Maturity Indicators**:
- **Basic**: Shared development environment (2-4 weeks setup)
- **Intermediate**: Dedicated pilot environment (1 week setup)
- **Advanced**: Self-service sandbox provisioning (hours)
- **Optimized**: Instant ephemeral environments with auto-teardown (minutes)

**Success Metrics**:
- Sandbox provision time (hours)
- Number of active pilots
- Pilot success rate (%)
- Time from pilot start to decision (weeks)

---

### 3.2 Feature Configuration & Customization

**Definition**: Configure and customize embedded AI features to align with BNZ processes, branding, and business rules.

**Key Components**:
- **Configuration Management**: Manage feature settings and parameters
- **Customization Options**: Tailor UI, workflows, and outputs
- **Business Rules**: Implement BNZ-specific logic and constraints
- **Prompt Engineering**: Optimize prompts for GenAI features
- **Model Fine-Tuning**: Where available, fine-tune on BNZ data
- **Feature Flagging**: Control feature availability by user group

**Technologies**:
- Configuration management tools
- Feature flag platforms (LaunchDarkly)
- Prompt management systems
- Model customization platforms

**Maturity Indicators**:
- **Basic**: Manual configuration with vendor support
- **Intermediate**: Documented configuration patterns and playbooks
- **Advanced**: Self-service configuration with templates
- **Optimized**: AI-assisted configuration recommendations

**Success Metrics**:
- Configuration time (days)
- Customization depth (%)
- Business rule compliance (%)
- Prompt effectiveness score

---

### 3.3 Enterprise Integration & SSO

**Definition**: Integrate embedded AI features with BNZ's enterprise systems, identity management, and security infrastructure.

**Key Components**:
- **Single Sign-On**: Azure AD/Okta integration for seamless access
- **API Integration**: Connect to core banking, CRM, and data systems
- **Data Synchronization**: Bi-directional data flows where required
- **Security Controls**: Apply BNZ security policies and access controls
- **Network Integration**: VPN, private endpoints, and network security
- **Audit Logging**: Capture all activity for compliance and security

**Technologies**:
- Identity providers (Azure AD, Okta)
- API management platforms
- Data integration tools
- SIEM systems
- Network security tools

**Maturity Indicators**:
- **Basic**: Manual integration with basic SSO (4-6 weeks)
- **Intermediate**: Standardized integration patterns (2-3 weeks)
- **Advanced**: Self-service integration with pre-built connectors (1 week)
- **Optimized**: Automatic integration via API discovery (days)

**Success Metrics**:
- SSO setup time (days)
- Integration reliability (%)
- Security incident rate (#)
- Audit log completeness (%)

---

### 3.4 Staged Rollout Management

**Definition**: Execute controlled, staged rollouts of embedded AI features to minimize risk and ensure smooth adoption.

**Key Components**:
- **Rollout Strategy**: Define phases, user cohorts, and success criteria
- **Phased Deployment**: Gradual expansion from pilot to full production
- **Blue-Green Deployment**: Maintain rollback capabilities
- **Canary Releases**: Test with small user groups before broad release
- **Feature Gates**: Control feature visibility by user segment
- **Rollback Plans**: Rapid rollback procedures for issues

**Technologies**:
- Feature management platforms
- Deployment automation tools
- User segmentation systems
- Monitoring and alerting platforms

**Maturity Indicators**:
- **Basic**: Big-bang deployment with basic rollback
- **Intermediate**: Phased rollout with defined stages
- **Advanced**: Automated staged rollout with dynamic gating
- **Optimized**: AI-driven rollout optimization and auto-rollback

**Success Metrics**:
- Rollout duration (days)
- Incident rate during rollout (#)
- Rollback frequency (#)
- User satisfaction by rollout phase

---

### 3.5 User Adoption & Training

**Definition**: Drive user adoption through targeted training, support, and change management to maximize value realization.

**Key Components**:
- **Training Programs**: Role-based training for different user types
- **Self-Service Resources**: Documentation, videos, and FAQs
- **Champions Network**: Power users who drive adoption in business units
- **Support Channels**: Help desk, chat support, and user communities
- **Adoption Metrics**: Track usage, engagement, and satisfaction
- **Feedback Loops**: Capture user feedback for continuous improvement

**Technologies**:
- Learning management systems
- Knowledge base platforms
- Support ticketing systems
- Adoption analytics tools
- User feedback platforms

**Maturity Indicators**:
- **Basic**: Ad-hoc training and documentation
- **Intermediate**: Structured training programs with resources
- **Advanced**: Multi-channel support with adoption tracking
- **Optimized**: Personalized learning paths with AI-driven recommendations

**Success Metrics**:
- User adoption rate (%)
- Training completion rate (%)
- Support ticket volume (#)
- User satisfaction score

---

## 4. Value Realization & Optimization

**Definition**: The capability to measure, track, and optimize the business value delivered by embedded AI features, ensuring continuous ROI and justifying ongoing investment.

**Purpose**: Prove and improve AI value delivery, creating virtuous cycle of investment and returns.

**Strategic Importance**: Converts AI from cost center to value driver, enabling additional investment and portfolio expansion.

### 4.1 Usage Analytics & Insights

**Definition**: Capture and analyze usage patterns, adoption metrics, and user behavior to understand how embedded AI features are being used.

**Key Components**:
- **Usage Tracking**: Monitor feature adoption, frequency, and user engagement
- **User Behavior Analysis**: Understand how users interact with AI features
- **Adoption Dashboards**: Visualize adoption trends across business units
- **Cohort Analysis**: Compare adoption and value across user segments
- **Feature Analytics**: Track which specific AI features drive most value
- **Anomaly Detection**: Identify usage patterns indicating issues or opportunities

**Technologies**:
- Analytics platforms (Google Analytics, Mixpanel)
- BI tools (Power BI, Tableau)
- User behavior analytics
- Product analytics platforms

**Maturity Indicators**:
- **Basic**: Basic usage reports from vendor dashboards
- **Intermediate**: Centralized analytics with custom reports
- **Advanced**: Real-time dashboards with predictive analytics
- **Optimized**: AI-driven insights and automated recommendations

**Success Metrics**:
- Daily/monthly active users
- Feature adoption rate (%)
- User engagement score
- Usage trend analysis

---

### 4.2 ROI Measurement & Tracking

**Definition**: Measure actual return on investment and track against projected benefits to validate business cases and inform future decisions.

**Key Components**:
- **Value Tracking**: Quantify productivity gains, cost savings, revenue impact
- **Cost Monitoring**: Track actual costs vs budget (licensing, support, infrastructure)
- **ROI Calculation**: Compare actual vs projected ROI
- **Value Attribution**: Link business outcomes to specific AI features
- **Benefit Realization**: Track achievement of projected benefits over time
- **Portfolio ROI**: Aggregate ROI across all embedded AI investments

**Technologies**:
- Financial tracking systems
- ROI calculators
- Benefit tracking platforms
- Portfolio management tools

**Maturity Indicators**:
- **Basic**: Annual ROI assessments with manual data collection
- **Intermediate**: Quarterly automated ROI reporting
- **Advanced**: Real-time ROI dashboards with drill-down analysis
- **Optimized**: Predictive ROI modeling with optimization recommendations

**Success Metrics**:
- Actual vs projected ROI (%)
- Payback period actual vs projected
- Total cost of ownership
- Value per user ($)

---

### 4.3 Performance Optimization

**Definition**: Continuously optimize embedded AI feature performance, including response times, accuracy, and business outcomes.

**Key Components**:
- **Performance Monitoring**: Track latency, throughput, and reliability
- **Quality Metrics**: Monitor AI output quality and accuracy
- **A/B Testing**: Test configuration changes and feature variations
- **Prompt Optimization**: Refine prompts for GenAI features
- **Configuration Tuning**: Optimize settings for business outcomes
- **Capacity Planning**: Ensure adequate resources for performance

**Technologies**:
- APM tools (Datadog, New Relic)
- A/B testing platforms
- Quality monitoring tools
- Capacity planning tools

**Maturity Indicators**:
- **Basic**: Reactive performance troubleshooting
- **Intermediate**: Proactive monitoring with alerts
- **Advanced**: Continuous optimization with A/B testing
- **Optimized**: AI-driven performance tuning and auto-scaling

**Success Metrics**:
- Response time (ms)
- AI output quality score
- User satisfaction with performance
- Cost per transaction

---

### 4.4 Feature Evolution & Roadmap Alignment

**Definition**: Track vendor feature roadmaps and ensure BNZ extracts maximum value from new capabilities as they become available.

**Key Components**:
- **Roadmap Tracking**: Monitor vendor product roadmaps and AI feature plans
- **Early Access Programs**: Participate in beta programs for strategic features
- **Feature Gap Analysis**: Identify gaps between current features and business needs
- **Upgrade Planning**: Plan and schedule upgrades to capture new capabilities
- **Retirement Planning**: Manage deprecation of legacy features
- **Innovation Harvesting**: Extract learnings from new features for broader application

**Technologies**:
- Vendor roadmap portals
- Product management tools
- Feature tracking systems
- Innovation management platforms

**Maturity Indicators**:
- **Basic**: React to vendor announcements
- **Intermediate**: Track roadmaps with proactive planning
- **Advanced**: Influence vendor roadmaps through strategic partnership
- **Optimized**: Predictive feature planning aligned to business strategy

**Success Metrics**:
- New feature adoption rate (%)
- Time from feature release to adoption (days)
- Feature utilization rate (%)
- Innovation capture rate (%)

---

### 4.5 Business Case Validation

**Definition**: Validate original business cases against actual outcomes to improve future investment decisions and build credibility.

**Key Components**:
- **Benefit Validation**: Confirm achievement of projected benefits
- **Assumption Testing**: Test and update assumptions from original business case
- **Lessons Learned**: Capture insights for future evaluations
- **Case Study Development**: Document successes for broader communication
- **Stakeholder Reporting**: Regular updates to business sponsors
- **Portfolio Optimization**: Reallocate resources based on validated performance

**Technologies**:
- Benefits realization platforms
- Case study tools
- Stakeholder reporting dashboards
- Portfolio analytics

**Maturity Indicators**:
- **Basic**: No systematic validation
- **Intermediate**: Annual validation with lessons learned
- **Advanced**: Quarterly validation with portfolio optimization
- **Optimized**: Continuous validation with real-time adjustments

**Success Metrics**:
- % of business cases validated
- Benefits realization rate (%)
- Forecast accuracy improvement (%)
- Investment reallocation based on validation

---

## 5. Operational Oversight (Light-Touch)

**Definition**: The capability to maintain appropriate operational oversight of embedded AI features with streamlined, risk-proportionate controls that don't impede velocity.

**Purpose**: Balance governance requirements with speed and autonomy, focusing oversight where it matters most.

**Strategic Importance**: Enables responsible innovation at scale without creating bureaucratic barriers.

### 5.1 Vendor Performance Monitoring

**Definition**: Track vendor performance against SLAs and contractual commitments with focus on material service providers.

**Key Components**:
- **SLA Tracking**: Monitor availability, performance, and support metrics
- **Incident Management**: Track and resolve vendor-caused incidents
- **Escalation Protocols**: Clear escalation paths for critical issues
- **Performance Dashboards**: Real-time visibility into vendor health
- **Material Provider Focus**: Enhanced monitoring for APRA CPS 230 material providers
- **Vendor Scorecards**: Periodic performance scoring and reviews

**Technologies**:
- Vendor management platforms
- SLA monitoring tools
- Incident tracking systems
- Performance analytics dashboards

**Maturity Indicators**:
- **Basic**: Manual SLA tracking with quarterly reviews
- **Intermediate**: Automated SLA monitoring with alerts
- **Advanced**: Real-time dashboards with predictive alerting
- **Optimized**: AI-driven vendor health prediction and auto-remediation

**Success Metrics**:
- SLA compliance rate (%)
- Mean time to resolution (hours)
- Vendor performance score
- Material provider oversight coverage (%)

---

### 5.2 Change Impact Assessment

**Definition**: Assess and manage the impact of vendor updates, configuration changes, and new feature rollouts with risk-based approaches.

**Key Components**:
- **Change Notification**: Track vendor change notifications and release notes
- **Impact Analysis**: Assess business, technical, and security impact
- **Testing Requirements**: Define testing based on change criticality
- **Change Windows**: Schedule updates during low-risk periods
- **Rollback Readiness**: Maintain ability to quickly revert changes
- **Communication**: Notify affected users of significant changes

**Technologies**:
- Change management systems
- Impact analysis tools
- Testing frameworks
- Communication platforms

**Maturity Indicators**:
- **Basic**: Manual change review with ad-hoc testing
- **Intermediate**: Standardized impact assessment with risk-based testing
- **Advanced**: Automated impact analysis with integrated testing
- **Optimized**: Predictive impact modeling with auto-classification

**Success Metrics**:
- Change assessment completion rate (%)
- Change-related incident rate (#)
- Average assessment cycle time (days)
- Rollback frequency (#)

---

### 5.3 Compliance Spot Checks

**Definition**: Conduct periodic spot checks to ensure ongoing compliance with policies, standards, and regulatory requirements without creating continuous audit burden.

**Key Components**:
- **Risk-Based Sampling**: Target spot checks based on risk profile
- **Automated Compliance Checks**: Where possible, automate policy compliance verification
- **Exception Management**: Track and resolve compliance exceptions
- **Audit Evidence**: Maintain evidence for regulatory examinations
- **Remediation Tracking**: Ensure timely resolution of compliance gaps
- **Trend Analysis**: Identify systemic compliance issues

**Technologies**:
- GRC platforms
- Compliance automation tools
- Audit management systems
- Policy management platforms

**Maturity Indicators**:
- **Basic**: Annual manual audits
- **Intermediate**: Quarterly spot checks with documentation
- **Advanced**: Continuous automated compliance monitoring
- **Optimized**: Predictive compliance risk identification

**Success Metrics**:
- Spot check coverage (%)
- Compliance finding rate (#)
- Remediation time (days)
- Audit readiness score

---

### 5.4 Incident Coordination

**Definition**: Coordinate response to incidents involving embedded AI features, working with vendors and internal teams to restore service and prevent recurrence.

**Key Components**:
- **Incident Classification**: Categorize incidents by severity and impact
- **Vendor Coordination**: Engage vendors for resolution support
- **Communication Plans**: Internal and external incident communication
- **Root Cause Analysis**: Conduct post-incident reviews
- **Lessons Learned**: Capture and apply learnings from incidents
- **Regulatory Notification**: Trigger APRA or RBNZ notifications when required

**Technologies**:
- Incident management platforms
- Communication tools
- Root cause analysis tools
- Regulatory reporting systems

**Maturity Indicators**:
- **Basic**: Reactive incident response with manual coordination
- **Intermediate**: Structured incident response with playbooks
- **Advanced**: Automated incident detection and orchestration
- **Optimized**: Predictive incident prevention with auto-mitigation

**Success Metrics**:
- Mean time to detect (minutes)
- Mean time to resolve (hours)
- Incident recurrence rate (%)
- Post-incident review completion rate (%)

---

## 6. Data & Privacy Enablement

**Definition**: The capability to enable safe data access and usage for embedded AI features while meeting privacy requirements and maintaining data quality.

**Purpose**: Unlock data as fuel for AI value while protecting privacy and maintaining trust.

**Strategic Importance**: Differentiates BNZ by enabling data-rich AI experiences within strict privacy boundaries.

### 6.1 Data Access Provisioning

**Definition**: Rapidly provision appropriate data access for embedded AI features with proper security controls and audit trails.

**Key Components**:
- **Access Request Process**: Streamlined requests for data access
- **Least Privilege**: Grant minimum necessary access for AI functionality
- **Data Classification**: Ensure appropriate controls based on data sensitivity
- **Access Reviews**: Periodic review of AI system data access
- **API Gateway**: Controlled access through standardized APIs
- **Audit Logging**: Comprehensive logging of all data access

**Technologies**:
- Identity and access management (IAM)
- API gateways
- Data classification tools
- Audit logging systems

**Maturity Indicators**:
- **Basic**: Manual access provisioning (weeks)
- **Intermediate**: Self-service access with approval workflows (days)
- **Advanced**: Automated access provisioning with policy-based controls (hours)
- **Optimized**: Dynamic access with real-time risk assessment (minutes)

**Success Metrics**:
- Access provisioning time (hours)
- Access review completion rate (%)
- Unauthorized access attempts (#)
- Audit log completeness (%)

---

### 6.2 Privacy Impact Assessment (Streamlined)

**Definition**: Conduct right-sized privacy impact assessments for embedded AI features, focusing on high-risk scenarios while enabling rapid deployment of low-risk features.

**Key Components**:
- **Risk-Based Thresholds**: Tiered PIA requirements based on data sensitivity
- **PIA Templates**: Pre-built templates for common embedded AI scenarios
- **Fast-Track Process**: Expedited review for low-risk features
- **Privacy by Design**: Build privacy controls into feature activation
- **Cross-Border Assessment**: Evaluate cross-border data flow risks
- **Privacy Register**: Maintain record of all PIAs conducted

**Technologies**:
- Privacy management platforms
- PIA workflow tools
- Data mapping tools
- Privacy register systems

**Maturity Indicators**:
- **Basic**: Full PIA for all features (4-6 weeks)
- **Intermediate**: Risk-tiered PIA with templates (1-2 weeks)
- **Advanced**: Automated PIA with fast-track for low-risk (days)
- **Optimized**: AI-assisted PIA with dynamic risk assessment (hours)

**Success Metrics**:
- PIA completion time (days)
- % of features with PIA coverage
- Privacy incident rate (#)
- Fast-track utilization rate (%)

---

### 6.3 Cross-Border Data Handling

**Definition**: Enable safe cross-border data flows for global SaaS platforms while meeting NZ Privacy Act and other jurisdictional requirements.

**Key Components**:
- **Data Residency Mapping**: Track where data is stored and processed
- **Transfer Mechanisms**: Standard contractual clauses, adequacy determinations
- **Localization Requirements**: Identify and implement data localization where required
- **Vendor Commitments**: Obtain contractual commitments for data protection
- **Transparency**: Disclose cross-border transfers to customers where required
- **Alternative Architectures**: Explore regional deployments for high-sensitivity data

**Technologies**:
- Data flow mapping tools
- Contract management systems
- Privacy compliance platforms
- Multi-region deployment tools

**Maturity Indicators**:
- **Basic**: Ad-hoc assessment of cross-border flows
- **Intermediate**: Documented data flow maps with transfer safeguards
- **Advanced**: Automated data flow discovery with compliance validation
- **Optimized**: Dynamic data routing based on regulatory requirements

**Success Metrics**:
- Data flow map completeness (%)
- Cross-border transfer compliance rate (%)
- Data localization coverage (%)
- Privacy complaint rate (#)

---

### 6.4 Data Quality for AI (IPP 8 Alignment)

**Definition**: Ensure data used by embedded AI meets accuracy and quality requirements, particularly for automated decisions under NZ Privacy Act IPP 8.

**Key Components**:
- **Accuracy Standards**: Define and enforce data accuracy requirements
- **Quality Monitoring**: Continuous monitoring of data quality metrics
- **Data Correction**: Processes to correct inaccurate data
- **Source Validation**: Verify data sources for AI consumption
- **Automated Decision Flagging**: Identify and track automated decisions
- **Quality Dashboards**: Visibility into data quality across AI systems

**Technologies**:
- Data quality platforms
- Data observability tools
- Master data management
- Quality monitoring dashboards

**Maturity Indicators**:
- **Basic**: Manual data quality checks
- **Intermediate**: Automated quality monitoring with alerts
- **Advanced**: Continuous quality monitoring with auto-correction
- **Optimized**: Predictive quality management with ML-driven validation

**Success Metrics**:
- Data accuracy rate (%)
- Quality incident rate (#)
- Automated decision accuracy (%)
- Data correction cycle time (hours)

**NZ Privacy Act IPP 8 Requirements**:
- Personal information used for automated decisions must be accurate, up-to-date, complete, and relevant
- Reasonable steps must be taken to ensure accuracy before use
- Individuals can request correction of personal information
- Banks must respond to correction requests within 20 working days

---

## 7. Portfolio & Ecosystem Management

**Definition**: The capability to manage the portfolio of embedded AI investments as a whole, optimizing across vendors and features to maximize strategic value.

**Purpose**: Ensure coherent, strategic approach to embedded AI rather than fragmented point solutions.

**Strategic Importance**: Drives economies of scale, reduces vendor sprawl, and creates strategic partnerships.

### 7.1 Portfolio Visibility & Governance

**Definition**: Maintain comprehensive visibility into all embedded AI features and investments across the organization.

**Key Components**:
- **AI Feature Registry**: Centralized catalog of all embedded AI deployments
- **Investment Tracking**: Track spending across all embedded AI features
- **Usage Analytics**: Aggregate usage across portfolio
- **Value Dashboard**: Portfolio-wide ROI and value metrics
- **Risk Dashboard**: Consolidated view of risks across embedded AI estate
- **Capability Coverage**: Map features to business capabilities

**Technologies**:
- Portfolio management platforms
- BI and analytics tools
- Asset management systems
- Dashboard platforms

**Maturity Indicators**:
- **Basic**: Spreadsheet tracking with manual updates
- **Intermediate**: Centralized registry with quarterly reviews
- **Advanced**: Real-time portfolio dashboard with analytics
- **Optimized**: AI-driven portfolio optimization recommendations

**Success Metrics**:
- Registry completeness (%)
- Portfolio ROI (%)
- Vendor consolidation ratio
- Feature overlap rate (%)

---

### 7.2 Strategic Vendor Relationships

**Definition**: Build strategic partnerships with key SaaS vendors to influence roadmaps, gain early access, and optimize commercial terms.

**Key Components**:
- **Vendor Segmentation**: Categorize vendors by strategic importance
- **Partnership Programs**: Engage in vendor advisory boards and early access programs
- **Roadmap Influence**: Provide input on vendor product direction
- **Executive Relationships**: Build executive-level vendor relationships
- **Commercial Optimization**: Negotiate portfolio-based pricing and terms
- **Co-Innovation**: Collaborate on industry-specific AI capabilities

**Technologies**:
- CRM systems for vendor relationships
- Collaboration platforms
- Contract management systems
- Innovation management tools

**Maturity Indicators**:
- **Basic**: Transactional vendor relationships
- **Intermediate**: Strategic relationships with top 3 vendors
- **Advanced**: Portfolio-wide partnerships with co-innovation
- **Optimized**: Industry-leading vendor partnerships with unique capabilities

**Success Metrics**:
- # of strategic vendor partnerships
- Early access program participation rate (%)
- Cost savings from portfolio agreements (%)
- Co-innovation projects (#)

---

### 7.3 Integration & Interoperability Patterns

**Definition**: Establish standard patterns for integrating embedded AI features and ensuring interoperability across the ecosystem.

**Key Components**:
- **Integration Catalog**: Library of reusable integration patterns
- **API Standards**: Standardized approaches to API integration
- **Data Exchange Patterns**: Common patterns for data flow between systems
- **Authentication Standards**: Consistent identity and access management
- **Monitoring Standards**: Uniform observability across embedded AI
- **Cross-Platform Workflows**: Enable workflows spanning multiple SaaS platforms

**Technologies**:
- Integration platforms (MuleSoft, Dell Boomi)
- API management
- Workflow orchestration tools
- Pattern libraries

**Maturity Indicators**:
- **Basic**: Custom integration per feature
- **Intermediate**: Documented integration patterns with reuse
- **Advanced**: Self-service integration with pattern library
- **Optimized**: Automated integration with AI-driven pattern recommendation

**Success Metrics**:
- Integration pattern reuse rate (%)
- Integration time reduction (%)
- Cross-platform workflow count (#)
- Interoperability score

---

### 7.4 Ecosystem Rationalization

**Definition**: Periodically review and rationalize the embedded AI portfolio, eliminating redundancy and optimizing the vendor ecosystem.

**Key Components**:
- **Capability Overlap Analysis**: Identify redundant capabilities across vendors
- **Consolidation Opportunities**: Identify opportunities to reduce vendor count
- **Exit Planning**: Systematically retire low-value or duplicate features
- **Migration Management**: Manage transitions between platforms
- **Cost Optimization**: Identify opportunities for cost reduction
- **Strategic Alignment**: Ensure portfolio alignment with business strategy

**Technologies**:
- Portfolio analytics platforms
- Cost optimization tools
- Migration planning tools
- Decision support systems

**Maturity Indicators**:
- **Basic**: Organic growth with no rationalization
- **Intermediate**: Annual portfolio review with ad-hoc consolidation
- **Advanced**: Continuous rationalization with active management
- **Optimized**: AI-driven portfolio optimization with automated recommendations

**Success Metrics**:
- Vendor count trend
- Feature redundancy reduction (%)
- Cost savings from consolidation ($)
- Portfolio efficiency score

---

## Success Metrics Summary

### Capability-Level KPIs

| Capability | Primary Metric | Target |
|-----------|---------------|--------|
| **SaaS AI Discovery & Evaluation** | Time from vendor release to evaluation | <14 days |
| **Vendor Enablement & Onboarding** | Average vendor onboarding time | 2-4 weeks |
| **Feature Activation & Deployment** | Time from contract to production | 4-6 weeks |
| **Value Realization & Optimization** | Actual vs projected ROI | >90% |
| **Operational Oversight** | SLA compliance rate | >99% |
| **Data & Privacy Enablement** | PIA completion time | <7 days |
| **Portfolio & Ecosystem Management** | Portfolio ROI | >119% (18 mo) |

### Business Value Metrics

- **Time to Value**: 3-6x faster than building in-house
- **Feature Adoption**: 70%+ of target users within 6 months
- **ROI Achievement**: 119% portfolio ROI within 18 months
- **User Satisfaction**: >4.5/5 average rating
- **Vendor Performance**: 95%+ SLA compliance
- **Innovation Velocity**: 20+ new AI features activated per year
- **Cost Efficiency**: 30-40% lower TCO vs custom build

---

## References

### Internal Standards
- **BNZ Architecture Principles**: `.standards/architecture/bnz-architecture-principles.md`
- **AI Platform Capability Model**: `../../01-capability/BNZ-AI-Platform-Capability-Model.md`
- **Governance Capability Model**: `../../06-governance/01-capability/governance-capabilities.md`
- **Knowledge Capability Model**: `../../04-knowledge/01-capability/knowledge-capabilities.md`

### Regulatory Frameworks
- **APRA CPS 230**: Operational Risk Management (material service provider oversight)
- **NZ Privacy Act 2020**: IPP 8 data accuracy requirements for automated decisions
- **EU AI Act**: Risk-based governance blueprint (12-month extension to August 2027 for embedded AI)
- **NIST AI RMF**: AI risk management framework
- **ISO/IEC 42001**: AI management systems standard

### Industry Resources
- **FS-ISAC**: Vendor risk assessment frameworks for financial services
- **Microsoft 365 Copilot Documentation**: Enterprise AI deployment guidance
- **Salesforce Einstein Documentation**: CRM-embedded AI best practices
- **ServiceNow AI Documentation**: IT service management AI features
- **Amazon Connect AI Documentation**: Customer service AI capabilities

### Technology References
- **API Management**: Apigee, Kong, Azure API Management
- **Integration Platforms**: MuleSoft, Dell Boomi, Azure Integration Services
- **Identity Management**: Azure AD, Okta
- **Observability**: Datadog, Splunk, New Relic
- **Portfolio Management**: ServiceNow SPM, Planview

---

**Copyright © 2025 Bank of New Zealand. All rights reserved.**  
**Owner**: BNZ Technology Strategy & Architecture | AI Platform - Embedded AI Capability Area
